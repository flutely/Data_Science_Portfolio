# Cross Validation

Have you ever wondered how well your statistical model is doing to model the data? One way to show model accuracy is using a method called cross validation.

## Overview

This page will cover the folowing:
- What is Cross Validation?
- \(k\)-fold Cross Validation
- Coding Cross Validation in Python
- Tradeoff between Bias and Variance

We'll use [Adult Income](https://www.kaggle.com/datasets/wenruliu/adult-income-dataset?resource=download) data as an example.

## What is Cross Validation?

Cross-validation is the process of splitting a dataset into two pieces and using one to fit a model, and the other piece to check the fit. 

## $k$-fold Cross Validation

There are a few steps to cross-validation

1) Split the data into $k$ folds

In this step we divide the data into $k$ sections

2) Train on $k-1$ folds

3) Validate on remaining fold

4) Repeat $k$ times

5) Average performance

## Coding Cross Validaion in Python

We'll start by loading the necessary libraries and the data.

```{python}
import numpy as np
import pandas as pd

adult_income = pd.read_csv("adult.csv")[['age', 'hours-per-week', 'class']]
```

We want to predict whether the income is greater than 50k for a given adult. 
```{python}
adult_income['income'] = (adult_income['class'] == '>50k').astype(int)

x = adult_income[['age', 'hours-per-week']].values
y = adult_income[['income]].values
```

Here we're going to use $k=5$ folds. 
```{python}
def k_fold_cv(x, y, k=5):
    n = len(x)
    indices = np.random.permutation(n)
    folds = np.array_split(indices, k)
    errors = []
    for i in range(k):
        test_idx = folds[i]
        train_idx = np.concatenate([folds[j] for j in range(k) if j != i])
        x_train, y_train = x[train_idx], y[train_idx]
        x_test, y_test = x[test_idx], y[test_idx]
        model = LinearRegression()
        model.fit(x_train, y_train)
        preds = model.predict(x_test)
        mse = mean_squared_error(y_test, preds)
        errors.append(mse)
    return np.mean(errors), np.std(errors)
```

## Tradeoff Between Bias and Variance